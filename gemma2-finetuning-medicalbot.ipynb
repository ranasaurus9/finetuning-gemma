{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Gemma2 9B base model on Medical dataset","metadata":{}},{"cell_type":"markdown","source":"## Installation\nInstall KerasNLP with the Gemma 2 model.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp tensorflow-text\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T10:26:06.760823Z","iopub.execute_input":"2024-07-30T10:26:06.761477Z","iopub.status.idle":"2024-07-30T10:27:41.203571Z","shell.execute_reply.started":"2024-07-30T10:26:06.761433Z","shell.execute_reply":"2024-07-30T10:27:41.202728Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Install huggingface datasets library","metadata":{}},{"cell_type":"code","source":"!pip install -q -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:27:49.505409Z","iopub.execute_input":"2024-07-30T10:27:49.505781Z","iopub.status.idle":"2024-07-30T10:27:57.085924Z","shell.execute_reply.started":"2024-07-30T10:27:49.505747Z","shell.execute_reply":"2024-07-30T10:27:57.084870Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Login to huggingface account","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:27:57.087765Z","iopub.execute_input":"2024-07-30T10:27:57.088161Z","iopub.status.idle":"2024-07-30T10:27:57.547610Z","shell.execute_reply.started":"2024-07-30T10:27:57.088127Z","shell.execute_reply":"2024-07-30T10:27:57.546818Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set up Keras JAX backend","metadata":{}},{"cell_type":"code","source":"import jax\n\njax.devices()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:12.977870Z","iopub.execute_input":"2024-07-30T10:29:12.978611Z","iopub.status.idle":"2024-07-30T10:29:12.983917Z","shell.execute_reply.started":"2024-07-30T10:29:12.978571Z","shell.execute_reply":"2024-07-30T10:29:12.983096Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# The Keras 3 distribution API is only implemented for the JAX backend for now\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:16.460058Z","iopub.execute_input":"2024-07-30T10:29:16.460679Z","iopub.status.idle":"2024-07-30T10:29:16.464379Z","shell.execute_reply.started":"2024-07-30T10:29:16.460645Z","shell.execute_reply":"2024-07-30T10:29:16.463592Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:19.017066Z","iopub.execute_input":"2024-07-30T10:29:19.017834Z","iopub.status.idle":"2024-07-30T10:29:27.425226Z","shell.execute_reply.started":"2024-07-30T10:29:19.017800Z","shell.execute_reply":"2024-07-30T10:29:27.424215Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Load model\n\nTo load the model with the weights and tensors distributed across TPUs, first create a new `DeviceMesh`. `DeviceMesh` represents a collection of hardware devices configured for distributed computation and was introduced in Keras 3 as part of the unified distribution API.\n\nThe distribution API enables data and model parallelism, allowing for efficient scaling of deep learning models on multiple accelerators and hosts. It leverages the underlying framework (e.g. JAX) to distribute the program and tensors according to the sharding directives through a procedure called single program, multiple data (SPMD) expansion. Check out more details in the new [Keras 3 distribution API guide](https://keras.io/guides/distribution/).","metadata":{}},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices(),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:30.765024Z","iopub.execute_input":"2024-07-30T10:29:30.765948Z","iopub.status.idle":"2024-07-30T10:29:30.769845Z","shell.execute_reply.started":"2024-07-30T10:29:30.765909Z","shell.execute_reply":"2024-07-30T10:29:30.769035Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"LayoutMap from the distribution API specifies how the weights and tensors should be sharded or replicated, using the string keys, for example, token_embedding/embeddings below, which are treated like regex to match tensor paths. Matched tensors are sharded with model dimensions (8 TPUs); others will be fully replicated.","metadata":{}},{"cell_type":"code","source":"model_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (model_dim, None)\n# Regex to match against the query, key and value matrices in attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value)/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*attention_output/kernel\"] = (model_dim, None, None)\nlayout_map[\"decoder_block.*ffw_gating.*/kernel\"] = (None, model_dim)\nlayout_map[\"decoder_block.*ffw_linear/kernel\"] = (model_dim, None)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:40.299409Z","iopub.execute_input":"2024-07-30T10:29:40.300246Z","iopub.status.idle":"2024-07-30T10:29:40.304718Z","shell.execute_reply.started":"2024-07-30T10:29:40.300209Z","shell.execute_reply":"2024-07-30T10:29:40.303883Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**ModelParallel** allows you to shard model weights or activation tensors across all devcies on the DeviceMesh. In this case, some of the Gemma 7B model weights are sharded across 8 TPU chips according the layout_map defined above. Now load the model in the distributed way.","metadata":{}},{"cell_type":"code","source":"model_parallel = keras.distribution.ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name=\"batch\",\n)\n\nkeras.distribution.set_distribution(model_parallel)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:44.086535Z","iopub.execute_input":"2024-07-30T10:29:44.086853Z","iopub.status.idle":"2024-07-30T10:29:44.091130Z","shell.execute_reply.started":"2024-07-30T10:29:44.086826Z","shell.execute_reply":"2024-07-30T10:29:44.090164Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now load the Gemma 2 27B model in the distributed way.","metadata":{}},{"cell_type":"code","source":"gemma2_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_9b_en\")\ngemma2_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:29:52.747778Z","iopub.execute_input":"2024-07-30T10:29:52.748136Z","iopub.status.idle":"2024-07-30T10:32:33.808925Z","shell.execute_reply.started":"2024-07-30T10:29:52.748098Z","shell.execute_reply":"2024-07-30T10:32:33.808075Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma2/keras/gemma2_9b_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,241,705,984\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Generate text before fine-tuning\n\nNow the Gemma 2 9B model is ready to be used for text generation.","metadata":{}},{"cell_type":"code","source":"print(gemma2_lm.generate(\"Hello doctor,Can I pull out hard teeth out no pain because I got a hole in my teeth? Whenever I use to eat something, it got stuck in it and caused severe pain. Kindly give me some advice about pulling it out without any pain.\", max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:33:03.171234Z","iopub.execute_input":"2024-07-30T10:33:03.171638Z","iopub.status.idle":"2024-07-30T10:33:57.441147Z","shell.execute_reply.started":"2024-07-30T10:33:03.171606Z","shell.execute_reply":"2024-07-30T10:33:57.440294Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Hello doctor,Can I pull out hard teeth out no pain because I got a hole in my teeth? Whenever I use to eat something, it got stuck in it and caused severe pain. Kindly give me some advice about pulling it out without any pain. I am 20 years old. I am a student. I am not able to eat anything. I am not able to sleep. I am not able to concentrate on my studies. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do anything. I am not able to do\n","output_type":"stream"}]},{"cell_type":"code","source":"print(gemma2_lm.generate(\"Hello doctor, The top of my belly button, that is, the skin of my belly button is hard and firm, and the rest of my belly button is not. I feel as if there is something under the surface. I changed my diet, so I lost some weight. I was 168 lbs, and now I am 135 lbs. If I place two fingers over my belly button and do wide circles, I feel like there is something hard underneath. Is this a hernia or where my umbilical cord was? I do not have pain or symptoms, but I just feel it firm and hard. What is this?\", max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:33:57.442506Z","iopub.execute_input":"2024-07-30T10:33:57.442822Z","iopub.status.idle":"2024-07-30T10:33:57.563910Z","shell.execute_reply.started":"2024-07-30T10:33:57.442788Z","shell.execute_reply":"2024-07-30T10:33:57.562940Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Hello doctor, The top of my belly button, that is, the skin of my belly button is hard and firm, and the rest of my belly button is not. I feel as if there is something under the surface. I changed my diet, so I lost some weight. I was 168 lbs, and now I am 135 lbs. If I place two fingers over my belly button and do wide circles, I feel like there is something hard underneath. Is this a hernia or where my umbilical cord was? I do not have pain or symptoms, but I just feel it firm and hard. What is this?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Dataset\nDownload medical chatbot dataset from huggingface","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"hf://datasets/ruslanmv/ai-medical-chatbot/dialogues.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:07.984008Z","iopub.execute_input":"2024-07-30T10:34:07.984749Z","iopub.status.idle":"2024-07-30T10:34:11.229935Z","shell.execute_reply.started":"2024-07-30T10:34:07.984711Z","shell.execute_reply":"2024-07-30T10:34:11.228688Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:11.231675Z","iopub.execute_input":"2024-07-30T10:34:11.231956Z","iopub.status.idle":"2024-07-30T10:34:11.244987Z","shell.execute_reply.started":"2024-07-30T10:34:11.231929Z","shell.execute_reply":"2024-07-30T10:34:11.243825Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                         Description  \\\n0      Q. What does abutment of the nerve root mean?   \n1  Q. What should I do to reduce my weight gained...   \n2  Q. I have started to get lots of acne on my fa...   \n\n                                             Patient  \\\n0  Hi doctor,I am just wondering what is abutting...   \n1  Hi doctor, I am a 22-year-old female who was d...   \n2  Hi doctor! I used to have clear skin but since...   \n\n                                              Doctor  \n0  Hi. I have gone through your query with dilige...  \n1  Hi. You have really done well with the hypothy...  \n2  Hi there Acne has multifactorial etiology. Onl...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Patient</th>\n      <th>Doctor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q. What does abutment of the nerve root mean?</td>\n      <td>Hi doctor,I am just wondering what is abutting...</td>\n      <td>Hi. I have gone through your query with dilige...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q. What should I do to reduce my weight gained...</td>\n      <td>Hi doctor, I am a 22-year-old female who was d...</td>\n      <td>Hi. You have really done well with the hypothy...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q. I have started to get lots of acne on my fa...</td>\n      <td>Hi doctor! I used to have clear skin but since...</td>\n      <td>Hi there Acne has multifactorial etiology. Onl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"med_records = df.sample(n=1000, random_state=2).to_dict('records')","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:16.984269Z","iopub.execute_input":"2024-07-30T10:34:16.984736Z","iopub.status.idle":"2024-07-30T10:34:17.004839Z","shell.execute_reply.started":"2024-07-30T10:34:16.984698Z","shell.execute_reply":"2024-07-30T10:34:17.003528Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"med_records[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:20.048813Z","iopub.execute_input":"2024-07-30T10:34:20.049212Z","iopub.status.idle":"2024-07-30T10:34:20.055473Z","shell.execute_reply.started":"2024-07-30T10:34:20.049180Z","shell.execute_reply":"2024-07-30T10:34:20.054310Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'Description': 'Is eloctrophoresis which shows a discrete band consistent with plasma cell dyscrasia anything to worry ?',\n 'Patient': 'i recently had a blood test because i was having a lot of pain which comes and goes i have a crush fracture of t9 blood tests are generally ok, except for the eloctrophoresis which shows a discrete band consistent with plasma cell dyscrasia such as myeloma or mgus',\n 'Doctor': 'your electrophoresis reports says that it is consistent with plasma cell dyscariasis. however not specified that it is myeloma or mgus. you need further investigation to confirm that what the disease you have. it depends on your monoclonal Ig levels and your symptoms. go for Monoclonal Ig levels,  bone marrow study, x-ray skull also needed. also scan you have any lytic lesion aor not. what is creatinine level and albumin level is also important. but one thing is sure you need further investigation for plasma cell dyscariasis. go for that and take treatment accordingly.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"Format medical dataset for fine-tuning","metadata":{}},{"cell_type":"code","source":"import json\ndata = []\n\ntemplate = \"\"\"Instruction:\n{Patient}\n\nResponse:\n{Doctor}\"\"\"\nfor record in med_records:\n    data.append(json.dumps(template.format(**record)))","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:23.401964Z","iopub.execute_input":"2024-07-30T10:34:23.402386Z","iopub.status.idle":"2024-07-30T10:34:23.418863Z","shell.execute_reply.started":"2024-07-30T10:34:23.402350Z","shell.execute_reply":"2024-07-30T10:34:23.417566Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:25.048485Z","iopub.execute_input":"2024-07-30T10:34:25.048842Z","iopub.status.idle":"2024-07-30T10:34:25.054623Z","shell.execute_reply.started":"2024-07-30T10:34:25.048811Z","shell.execute_reply":"2024-07-30T10:34:25.053556Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\"Instruction:\\\\ni recently had a blood test because i was having a lot of pain which comes and goes i have a crush fracture of t9 blood tests are generally ok, except for the eloctrophoresis which shows a discrete band consistent with plasma cell dyscrasia such as myeloma or mgus\\\\n\\\\nResponse:\\\\nyour electrophoresis reports says that it is consistent with plasma cell dyscariasis. however not specified that it is myeloma or mgus. you need further investigation to confirm that what the disease you have. it depends on your monoclonal Ig levels and your symptoms. go for Monoclonal Ig levels,  bone marrow study, x-ray skull also needed. also scan you have any lytic lesion aor not. what is creatinine level and albumin level is also important. but one thing is sure you need further investigation for plasma cell dyscariasis. go for that and take treatment accordingly.\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"## LoRA Fine-tuning\n\nTo get better responses from the model, you can fine-tune the model with Low Rank Adaptation (LoRA) using the Databricks Dolly 15k dataset.\n\nThe LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.\n\nA higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n\nThis tutorial uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma2_lm.backbone.enable_lora(rank=4)\ngemma2_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:28.789682Z","iopub.execute_input":"2024-07-30T10:34:28.790548Z","iopub.status.idle":"2024-07-30T10:34:29.740976Z","shell.execute_reply.started":"2024-07-30T10:34:28.790505Z","shell.execute_reply":"2024-07-30T10:34:29.739971Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)        │   \u001b[38;5;34m9,256,242,688\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m917,504,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,256,242,688</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">917,504,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,256,242,688\u001b[0m (34.48 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,256,242,688</span> (34.48 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,536,704\u001b[0m (55.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,536,704</span> (55.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,241,705,984\u001b[0m (34.43 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,241,705,984</span> (34.43 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"Note that enabling LoRA reduces the number of trainable parameters significantly (from 9 billion to 14 million).","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma2_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma2_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:40.218977Z","iopub.execute_input":"2024-07-30T10:34:40.219453Z","iopub.status.idle":"2024-07-30T10:34:40.380044Z","shell.execute_reply.started":"2024-07-30T10:34:40.219417Z","shell.execute_reply":"2024-07-30T10:34:40.378727Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Start fine-tuning job","metadata":{}},{"cell_type":"code","source":"gemma2_lm.fit(data, epochs=10, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:34:43.683519Z","iopub.execute_input":"2024-07-30T10:34:43.683918Z","iopub.status.idle":"2024-07-30T10:46:50.372186Z","shell.execute_reply.started":"2024-07-30T10:34:43.683884Z","shell.execute_reply":"2024-07-30T10:46:50.371219Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 289ms/step - loss: 7.0965 - sparse_categorical_accuracy: 0.4179\nEpoch 2/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 2.0822 - sparse_categorical_accuracy: 0.4465\nEpoch 3/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 1.9817 - sparse_categorical_accuracy: 0.4642\nEpoch 4/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 1.9552 - sparse_categorical_accuracy: 0.4683\nEpoch 5/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 259ms/step - loss: 1.9378 - sparse_categorical_accuracy: 0.4704\nEpoch 6/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 261ms/step - loss: 1.9212 - sparse_categorical_accuracy: 0.4735\nEpoch 7/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 1.9030 - sparse_categorical_accuracy: 0.4766\nEpoch 8/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 259ms/step - loss: 1.8808 - sparse_categorical_accuracy: 0.4811\nEpoch 9/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 1.8527 - sparse_categorical_accuracy: 0.4860\nEpoch 10/10\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 260ms/step - loss: 1.8176 - sparse_categorical_accuracy: 0.4936\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78c07bfc3190>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Generate text after fine-tuning\nAfter fine-tuning, responses follow the instruction provided in the prompt.","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    Patient=\"i recently had a blood test because i was having a lot of pain which comes and goes i have a crush fracture of t9 blood tests are generally ok, except for the eloctrophoresis which shows a discrete band consistent with plasma cell dyscrasia such as myeloma or mgus. Is it a thing to worry ?\",\n    Doctor=\"\",\n)\nprint(gemma2_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:46:54.127823Z","iopub.execute_input":"2024-07-30T10:46:54.129031Z","iopub.status.idle":"2024-07-30T10:47:49.370669Z","shell.execute_reply.started":"2024-07-30T10:46:54.128981Z","shell.execute_reply":"2024-07-30T10:47:49.369525Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Instruction:\ni recently had a blood test because i was having a lot of pain which comes and goes i have a crush fracture of t9 blood tests are generally ok, except for the eloctrophoresis which shows a discrete band consistent with plasma cell dyscrasia such as myeloma or mgus. Is it a thing to worry ?\n\nResponse:\nHello, Thank you for posting your query. I have gone through your query and understand your concern. I would like to inform you that the discrete band in electrophoresis is suggestive of monoclonal gammopathy. This is a condition in which there is an increase in the level of a particular type of protein in the blood. This is usually seen in multiple myeloma. So, you need to get a bone marrow biopsy done to rule out multiple myeloma. Hope I have answered your query. I will be happy to help you further. Wish you good health. Thanks.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    Patient=\"\"\"my daughter 9 yrs old went to community swimming pool for 3 days there .she developed some skin infection around her right eye and on her chin and lip. both areas have got inflamed and she is complaining of pain and itching. now my question is whether her condition can be treated by only topical application or she needs oral medication too.\n    \"\"\",\n    Doctor=\"\",\n)\nprint(gemma2_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-07-30T10:57:26.152829Z","iopub.execute_input":"2024-07-30T10:57:26.153524Z","iopub.status.idle":"2024-07-30T10:58:21.671375Z","shell.execute_reply.started":"2024-07-30T10:57:26.153479Z","shell.execute_reply":"2024-07-30T10:58:21.670231Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Instruction:\nmy daughter 9 yrs old went to community swimming pool for 3 days there .she developed some skin infection around her right eye and on her chin and lip. both areas have got inflamed and she is complaining of pain and itching. now my question is whether her condition can be treated by only topical application or she needs oral medication too.\n    \n\nResponse:\nHi, Thanks for posting your query. I have gone through your query and understand your concern. I would suggest you to apply topical antibiotic ointment like mupirocin ointment 2% on the affected area. You can also apply topical steroid ointment like mometasone furoate 0.1% on the affected area. You can also apply topical antihistamine ointment like\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save finetuned model to Kaggle/HuggingFace","metadata":{}},{"cell_type":"code","source":"# Save the finetuned model as a KerasNLP preset.\ngemma.save_to_preset(\"./gemma2-medical-base-7b\")\n\n# Upload the preset as a new model variant on Kaggle\nkaggle_uri = \"kaggle://my_kaggle_username/gemma-medical/keras/gemma2-medical-base-7b\"\nkeras_nlp.upload_preset(kaggle_uri, \"./gemma2-medical-base-7b\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then save it as a KerasNLP preset.\ngemma2_lm.save_to_preset('./gemma2-medical-base-7b)\n\n# Upload the preset to Hugging Face Hub\nhf_uri = \"hf://my_hf_username/gemma2-medical-base-7b\"\nkeras_nlp.upload_preset(hf_uri, './gemma2-medical-base-7b)","metadata":{},"execution_count":null,"outputs":[]}]}